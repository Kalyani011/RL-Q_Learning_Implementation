{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CT5134: Agents, Multi-Agent Systems, and Reinforcement Learning \n",
    "\n",
    "## Assignment 2: Q-Learning Implementation for FrozenLake Problem \n",
    "\n",
    "#### **Submitted By**: Kalyani Prashant Kawale\n",
    "#### **Student ID**: 21237189"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Create the FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Setting up FrozenLake grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising grid, start state, goal state and hole states\n",
    "LAKE_ROWS = 5\n",
    "LAKE_COLUMNS = 5\n",
    "START_STATE = (0, 0)\n",
    "GOAL_STATE = (5, 5)\n",
    "HOLES = [(1,0), (1,3), (3,1), (4,2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 State Configurations for the FrozenLake Grid Environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    # Initialising the state with START_STATE\n",
    "    def __init__(self, state=START_STATE):    \n",
    "        self.state = state\n",
    "#         self.isEnd = False        \n",
    "    \n",
    "    # Method to get the rewards for state transitions\n",
    "    def get_reward(self):\n",
    "        if self.state == GOAL_STATE:\n",
    "            return 10\n",
    "        elif self.state in HOLES:\n",
    "            return -5\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    # Method to check if agent is in end state,\n",
    "    # where end state could be the goal state or any of the hole states\n",
    "    def is_end(self):\n",
    "        if (self.state == GOAL_STATE) or (self.state in HOLES):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    # Method to determine the next state of the agent based on the action taken\n",
    "    def next_position(self, action):\n",
    "        # if action up, move one row up, keeping the column same\n",
    "        if action == 0:                \n",
    "            next_state = (self.state[0] - 1, self.state[1])\n",
    "        # if action down, move one row down, keeping the column same \n",
    "        elif action == 1:\n",
    "            next_state = (self.state[0] + 1, self.state[1])\n",
    "        # if action left, move one column back, keeping the row same \n",
    "        elif action == 2:\n",
    "            next_state = (self.state[0], self.state[1] - 1)\n",
    "        # if action right (action=3), move one column forward, keeping the row same \n",
    "        else:\n",
    "            next_state = (self.state[0], self.state[1] + 1)\n",
    "        # if next state is within the 5x5 grid of FrozenLake returning the next state\n",
    "        if (next_state[0] >= 0) and (next_state[0] <= 4):\n",
    "            if (next_state[1] >= 0) and (next_state[1] <= 4):                    \n",
    "                    return next_state # if next state legal\n",
    "        # keeping state unchanged for move off the grid\n",
    "        return self.state "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implement the Reinforcement Learning algorithm Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Setting up Agent Configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = [0, 1, 2, 3] # up, down, left, right\n",
    "        self.State = State()\n",
    "        self.alpha = 0.5\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 0.1\n",
    "        \n",
    "        # initialising the action value estimates to 0\n",
    "        self.Q = {}        \n",
    "        for row in range(LAKE_ROWS):\n",
    "            for col in range(LAKE_COLUMNS):\n",
    "                for action in range(len(self.actions)):\n",
    "                    self.Q[(row, col, action)] = 0  # set initial value to 0, for Q(s,a)\n",
    "                \n",
    "        self.new_Q = {}\n",
    "            \n",
    "    def Q_Learning(self, episodes):\n",
    "        for episode in range(episodes):\n",
    "            for row in range(LAKE_ROWS):\n",
    "                for col in range(LAKE_COLUMNS):\n",
    "                    self.State = State(state=(row, col))\n",
    "                    if(not self.State.is_end()):\n",
    "                        max_value = -5\n",
    "                        if np.random.random() < self.epsilon:\n",
    "                            for action in self.actions:\n",
    "                                if self.Q[(row, col, action)] >= max_value:                             \n",
    "                                    max_action = action\n",
    "                                    max_value = self.Q[(row, col, action)]\n",
    "                            action = max_action \n",
    "                        else:\n",
    "                            action = np.random.randint(4)\n",
    "                        self.Q \n",
    "                    else:\n",
    "                        mx_nxt_value = self.State.getReward()\n",
    "        \n",
    "    def value_iteration(self, episodes):\n",
    "        # Value iteration implementation                \n",
    "        x = 0\n",
    "        # Loop through all the states rows * cols        \n",
    "        while x < episodes:\n",
    "            i = 0\n",
    "            j = 0\n",
    "            k = 0\n",
    "            while i < BOARD_ROWS:\n",
    "                while j < BOARD_COLS: # for all states\n",
    "                    while k < len(self.actions): # for all actions\n",
    "                        mx_nxt_value = -5\n",
    "                        self.State = State(state=(i, j)) \n",
    "                        self.State.isEndFunc()\n",
    "                        if(not self.State.isEnd):\n",
    "                            for a in self.actions: # Loop to discover max_a' Q(s',a')\n",
    "                                nxtState = self.State.nxtPosition(a)\n",
    "                                nxtStateAction = (nxtState[0], nxtState[1], a)\n",
    "                                nxt_value = self.State.getReward() + (self.discount * self.action_values[nxtStateAction]) # Bellman equation Q(s,a) <- r(s,a) + gamma (Q(s'a'))\n",
    "                                if nxt_value >= mx_nxt_value:\n",
    "                                    mx_nxt_value = nxt_value\n",
    "                        else:\n",
    "                            mx_nxt_value = self.State.getReward()\n",
    "                        # Update the state values\n",
    "                        self.new_action_values[(i,j,k)] = round(mx_nxt_value,3)\n",
    "                        k += 1\n",
    "                    j += 1\n",
    "                    k = 0\n",
    "                i += 1\n",
    "                j = 0                \n",
    "            x += 1\n",
    "            self.action_values = self.new_action_values.copy()\n",
    "            \n",
    "    # Method to display action value estimates\n",
    "    def showValues(self):\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('---------------------------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                mx_nxt_value = -5\n",
    "                for a in self.actions:\n",
    "                    nxt_value = self.action_values[(i,j,a)]\n",
    "                    if nxt_value >= mx_nxt_value:\n",
    "                        mx_nxt_value = nxt_value\n",
    "                out += str(mx_nxt_value).ljust(5) + ' | '\n",
    "            print(out)\n",
    "        print('---------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
